# SuperAGI
Assignment

# Task-0
the file named as Asssignment_AI.pdf is  Q/A Assignment

# Task-1
GPT2-small model (with 125 million parameters) model has downloaded form hugging face . Make sure you touch upon the key aspects of the model like multi-head self-attention mechanism, feed-forward networks and positional encoding.

## Stage-1: 
 start with basic gpt2 model using gpt-2(small) and predicting the ouput, the tokenizier used in this is same gpt-2
![image](https://github.com/saikumar2882/SuperAGI/assets/76247735/319451bc-8eed-40e1-b6e1-cda3e0454c5f)

## stage-2
  Downloaded the gpt-2 small model and using same as tokenizier and predicting the ouput and able to fine tune on our own dataset and able to produce ouput
### step-1:
download the model and confiuration and tokenizer file:
![image](https://github.com/saikumar2882/SuperAGI/assets/76247735/32fa1973-c0de-4fb7-bd7a-4f60f8d8b233)
### step-2:
load the model and fine tune the model an your own data set and predict the output:
  * model = GPT2LMHeadModel.from_pretrained('/home/tejavathsaikumar/models/gpt2') 
  * config = GPT2Config.from_pretrained('/home/tejavathsaikumar/models/gpt2')
  * model.resize_token_embeddings(len(tokenizer))
explanation:

model = GPT2LMHeadModel.from_pretrained('/home/tejavathsaikumar/models/gpt2'): This line loads a pre-trained GPT-2 language model from the specified directory (/home/tejavathsaikumar/models/gpt2). The from_pretrained method initializes the GPT2LMHeadModel with pre-trained weights and architecture from the given path.

config = GPT2Config.from_pretrained('/home/tejavathsaikumar/models/gpt2'): This line loads the configuration file associated with the GPT-2 model from the same directory path. The from_pretrained method is used here to load the model's configuration.

model.resize_token_embeddings(len(tokenizer)): This line resizes the token embeddings of the loaded model to match the vocabulary size of the provided tokenizer. The tokenizer variable should already be defined and initialized with the tokenizer corresponding to the GPT-2 model.

By setting the token embeddings' size to match the tokenizer's vocabulary size, the model can properly process and understand tokens generated by the tokenizer.

The provided code tells that the directory /home/tejavathsaikumar/models/gpt2 contains the necessary files for the GPT-2 model (including the model weights, configuration, and other related files).

### step-3:
![image](https://github.com/saikumar2882/SuperAGI/assets/76247735/cd8641be-2838-475b-a48b-3b3fb8154454)
the above is used to predict the ouput and print in the console
